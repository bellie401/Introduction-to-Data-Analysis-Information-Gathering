{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import os\n","import time\n","from math import sqrt\n","import numpy as np\n","import pandas as pd\n","import lightgbm as lgb\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":["## Load Data"]},{"metadata":{"trusted":true},"cell_type":"code","source":["train = pd.read_csv('../input/duth-dbirlab2-1/train.csv')\n","test = pd.read_csv('../input/duth-dbirlab2-1/test.csv')"],"execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Encode Categorical"]},{"metadata":{},"cell_type":"markdown","source":["Χρήση παραλλαγής της κωδικοποίησης categorical features, ώστε να έχουμε πιο εύλογη αντικατάσταση των ειδών του εδάφους και όχι τυχαία ανάθεση τιμών"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Custom substrate labelling\n","for i in range (0,train['substrateType'].values.size):\n","    if train['substrateType'].values[i] == \"Unknown\":\n","        train['substrateType'].values[i] = 0\n","    if train['substrateType'].values[i] == \"Rock or other hard substrata\":\n","        train['substrateType'].values[i] = 1\n","    if train['substrateType'].values[i] == 'Coarse and mixed sediment':\n","        train['substrateType'].values[i] = 2\n","    if train['substrateType'].values[i] == 'Fine mud':\n","        train['substrateType'].values[i] = 3\n","    if train['substrateType'].values[i] == 'Sandy mud':\n","        train['substrateType'].values[i] = 4\n","    if train['substrateType'].values[i] == 'Muddy sand':\n","        train['substrateType'].values[i] = 5\n","    if train['substrateType'].values[i] == 'Sand':\n","        train['substrateType'].values[i] = 6\n","    if train['substrateType'].values[i] == 'Cymodocea nodosa meadows':\n","        train['substrateType'].values[i] = 7\n","    if train['substrateType'].values[i] == 'Posidonia oceanica meadows':\n","        train['substrateType'].values[i] = 8\n","      "],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Πρόχειρη διαγραφή τιμών outiers και συγκεκριμένα αυτών με πολύ μεγάλη τιμή, δηλαδή μεγαλύτερη της 10^5"]},{"metadata":{"trusted":true},"cell_type":"code","source":["excludeList = []\n","for col in train.columns:    \n","    r = train[col]\n","    a = np.array(r)\n","#     if abs(np.percentile(a,100))>1000*abs(np.percentile(a,99)):\n","    for i in range(0,len(a)):\n","        if abs(a[i])>1e5:\n","            excludeList.append(i)\n","            train[col].values[i] = np.median(a)\n","print(excludeList)\n","print(len(excludeList))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Παραλλαγή κλασσικής μεθόδου feature engineering binning, προσασμοσμένη στα δεδομένα μας"]},{"metadata":{"trusted":true},"cell_type":"code","source":["#custom binning\n","def customBin(element,df,lb,ub):\n","    index = df.columns.get_loc(element)\n","    new_list = []\n","    for row in df.values:\n","        if row[index]>lb and row[index]<ub:\n","            new_list.append(1)\n","        else:\n","            new_list.append(0) \n","    \n","    new_df = pd.DataFrame(data=new_list,columns=[element+\"_cb\"])\n","    return new_df"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Ακολουθεί και η εφαρμογή της μεθόδου binning σε συγκεκριμένες στήλες μόνο του dataset, όπως καταλήξαμε ότι συνίσταται από την έρευνα μας"]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(train.shape)\n","cb = customBin('temperatureSurface_quantile_5',train,6,16)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('temperatureSurface_quantile_5',test,6,16)\n","test = pd.concat([test,cb],axis=1)\n","\n","cb = customBin('Center Lat',train,32.5,46)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('Center Lat',test,32.5,46)\n","test = pd.concat([test,cb],axis=1)\n","\n","cb = customBin('majorRiversScale',train,6,10)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('majorRiversScale',test,6,10)\n","test = pd.concat([test,cb],axis=1)\n","\n","cb = customBin('Center Long',train,-10,26)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('Center Long',test,-10,26)\n","test = pd.concat([test,cb],axis=1)\n","\n","cb = customBin('dissolvedOxygen100_300_Moving_skew_3_mean',train,-10,-4)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('dissolvedOxygen100_300_Moving_skew_3_mean',test,-10,-4)\n","test = pd.concat([test,cb],axis=1)\n","\n","cb = customBin('dissolvedOxygenSurface_Moving_skew_6_mean',train,-5,-2)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('dissolvedOxygenSurface_Moving_skew_6_mean',test,-5,-2)\n","test = pd.concat([test,cb],axis=1)\n","\n","cb = customBin('nitrate100_300_Expanding_skew_mean',train,-10,-4)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('nitrate100_300_Expanding_skew_mean',test,-10,-4)\n","test = pd.concat([test,cb],axis=1)\n","\n","cb = customBin('secchiDiskDepth_Moving_skew_3_mean',train,-150,-80)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('secchiDiskDepth_Moving_skew_3_mean',test,-150,-80)\n","test = pd.concat([test,cb],axis=1)\n","\n","cb = customBin('bathymetry',train,0,-400)\n","train = pd.concat([train,cb],axis=1)\n","cb = customBin('bathymetry',test,0,-400)\n","test = pd.concat([test,cb],axis=1)\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Στην συνέχεια της εφαρμογής του feature selection μας, θα επιλέξουμε να κρατήσουμε μόνο συγκεκριμένες στήλες απο το dataframe μας. Αρχικά κάνουμε drop όσες στήλες έχουνε numerical correlation με το ζητούμενο μας μικρότερο του 0.3 "]},{"metadata":{"trusted":true},"cell_type":"code","source":["num=train.select_dtypes(exclude='object')\n","numcorr=num.corr()\n","Num=abs(numcorr['Overall Probability']).sort_values(ascending=True)\n","\n","NumF=Num[Num<0.3]\n","npd = NumF.to_frame()\n","npd = npd.transpose()\n","cols_to_exclude = list(npd.columns)\n","print(len(cols_to_exclude))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Σε συνεχεια του προηγούμενου block, επιλέγουμε να ρίξουμε ενδεικτικά τις 1000 στήλες με τις χειρότερες τυπικές αποκλίσεις"]},{"metadata":{"trusted":true},"cell_type":"code","source":["stds_train=train.std(axis=0)\n","indexes = np.argsort(stds_train)\n","lim = 0\n","# cols_to_exclude = []\n","for i in indexes:\n","    lim +=1\n","    if lim > 1000:\n","        break\n","    cols_to_exclude.append(train.columns[i])\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["for df in [train,test]:\n","    for c in df.drop(['obs_id'],axis=1):\n","        if (df[c].dtype=='object'):\n","            lbl = LabelEncoder() \n","            lbl.fit(list(df[c].values))\n","            df[c] = lbl.transform(list(df[c].values))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Ακολουθεί μία μέθοδος optimization των μεγεθών dataframes, για βελτιστοποίηση της προσπέλασης τους"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def reduce_mem_usage(df):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.        \n","    \"\"\"\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","    \n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        \n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        else:\n","            df[col] = df[col].astype('category')\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Train Models"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Some useful parameters which will come in handy later on\n","ntrain = train.shape[0] # or len(train)\n","ntest = test.shape[0] # or len(test)\n","SEED = 3999 # for reproducibility\n","NFOLDS = 10 # set folds for out-of-fold prediction\n","folds = KFold(n_splits= NFOLDS, random_state=SEED, shuffle=True)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# cols_to_exclude = ['obs_id','Overall Probability','median','quantile_1','quantile_10','quantile_90','quantile_99']\n","cols_to_exclude.append('skew')\n","cols_to_exclude.append('std')\n","cols_to_exclude.append('kurt')\n","cols_to_exclude.append('median')\n","cols_to_exclude.append('dissolvedOxygenSurface_mad')\n","cols_to_exclude.append('euphoticDepth_mad')\n","cols_to_exclude.append('euphoticDepthiqr')\n","cols_to_exclude.append('euphoticDepthiqr1')\n","cols_to_exclude.append('quantile')\n","\n","cols_to_exclude_default = ['obs_id','Overall Probability']\n","dftc = []\n","ex = False\n","for c in train.columns:\n","    for col2e in cols_to_exclude:\n","        if col2e in c:\n","            ex = True\n","    \n","    if not ex:\n","        dftc.append(c)\n","    else:\n","        ex = False\n","        \n","dftc = [c for c in dftc if c not in cols_to_exclude_default]\n","dftc.append('distanceToCoast')\n","dftc.append('majorRiversScale')\n","dftc.append('temperatureSurface_quantile_5_cb')\n","dftc.append('Center Lat_cb')\n","dftc.append('Center Long_cb')\n","dftc.append('majorRiversScale_cb')\n","\n","\n","# r_train = reduce_mem_usage(train)\n","# r_test = reduce_mem_usage(test)\n","        \n","y_train = train['Overall Probability'].ravel() #ravel coverts a series to a numpy array\n","x_train = train[dftc].values # converts a dataframe to a numpy array\n","x_test = test[dftc].values\n","print(x_train.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(dftc)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def train_model(X_train, X_test, Y_train, folds=5, model_type='lgb',plot_feature_importance=True):\n","\n","    oof = np.zeros(ntrain)\n","    prediction = np.zeros(ntest)\n","    scores = []\n","    feature_importance = pd.DataFrame()\n","    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train,Y_train)):\n","        print('Fold', fold_n+1, 'started at', time.ctime())\n","        x_train, x_valid = X_train[train_index], X_train[valid_index]\n","        y_train, y_valid = Y_train[train_index], Y_train[valid_index]      \n","        \n","        if model_type == 'linear':\n","            model = LinearRegression()\n","            model.fit(x_train, y_train)\n","            y_pred_valid = model.predict(x_valid)\n","            y_pred = model.predict(X_test) \n","            \n","        if model_type == 'rf':\n","            model = RandomForestRegressor(min_weight_fraction_leaf=0.05,n_jobs=-2,random_state=0,\n","                                          max_depth=4,\n","                                         n_estimators=100)\n","            model.fit(x_train, y_train)\n","            y_pred_valid = model.predict(x_valid)\n","            y_pred = model.predict(X_test)               \n","        \n","        if model_type == 'lgb':\n","            lgb_params = {   \n","                         'num_leaves':20,\n","                         'min_data_in_leaf': 20,\n","                         'min_sum_hessian_in_leaf': 11,\n","                         'objective': 'regression',\n","                         'max_depth': 20,\n","                         'learning_rate': 0.005,\n","                         'boosting': \"gbdt\",\n","                         'feature_fraction': 0.8,\n","                         'feature_fraction_seed': 9,\n","                         'max_bin ': 1000,\n","                         \"bagging_freq\": 5,\n","                         \"bagging_fraction\": 0.8,\n","                         \"bagging_seed\": 9,\n","                         'metric': 'rmse',\n","                         'lambda_l1': 0.1,\n","                         'verbosity': -1,\n","                         'min_child_weight': 5.34,\n","                         'reg_alpha': 1.130,\n","                         'reg_lambda': 0.360,\n","                         'subsample': 0.8,\n","                         }\n","            \n","            \n","            model = lgb.LGBMRegressor(**lgb_params, n_estimators = 100000, n_jobs = -1)\n","            model.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_valid, y_valid)], eval_metric='rmse',verbose=10000, early_stopping_rounds=1000)\n","            \n","            y_pred_valid = model.predict(x_valid)\n","            y_pred_valid = np.clip(y_pred_valid, a_min=0, a_max=1)\n","            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n","            y_pred = np.clip(y_pred, a_min=0, a_max=1)\n","            \n","            # feature importance\n","            fold_importance = pd.DataFrame()\n","            fold_importance[\"feature\"] = train[dftc].columns\n","            fold_importance[\"importance\"] = model.feature_importances_\n","            fold_importance[\"fold\"] = fold_n + 1\n","            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)            \n","\n","        \n","        oof[valid_index] = y_pred_valid.reshape(-1,)\n","        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n","        prediction += y_pred          \n","        \n","    if (model_type == 'lgb' and plot_feature_importance==True):\n","\n","        cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n","            by=\"importance\", ascending=False)[:50].index\n","\n","        best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n","\n","        plt.figure(figsize=(16, 12));\n","        sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n","        plt.title('LGB Features (avg over folds)')\n","\n","    prediction /= NFOLDS        \n","    print('CV mean score: {0:.5f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n","    \n","\n","    \n","    return oof, prediction"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["oof, prediction = train_model(X_train=x_train, X_test=x_test, Y_train=y_train, folds=folds, model_type='lgb', plot_feature_importance=True)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Create Submission File"]},{"metadata":{"trusted":true},"cell_type":"code","source":["sample_submission = pd.read_csv('../input/duth-dbirlab2-1/sample_submission.csv')\n","sub_df = pd.DataFrame({\"obs_id\":sample_submission[\"obs_id\"].values})\n","sub_df[\"Overall Probability\"] = prediction\n","sub_df[\"Overall Probability\"] = sub_df[\"Overall Probability\"].apply(lambda x: 1 if x>1 else 0 if x<0 else x)\n","sub_df.to_csv(\"submission.csv\", index=False)"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}